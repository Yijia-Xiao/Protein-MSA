{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# raw_megatron = torch.load('/workspace/ckpt/release/768h-12l-12hd-1mbs-512gbs-1mp-16384tokens-256aligns-1024length-1600ws-100000iter-release/iter_0000500/mp_rank_00/model_optim_rng.pt')\n",
    "raw_megatron = torch.load('/dataset/ee84df8b/release/ProteinLM/pretrain/ckpt/iter_0000005/mp_rank_00/model_optim_rng.pt')\n",
    "# raw_esm = torch.load('/root/.cache/torch/hub/checkpoints/esm_msa1b_t12_100M_UR50S.pt')\n",
    "raw_esm = torch.load('/dataset/ee84df8b/release/ProteinLM/pretrain/data/esm_msa1b_t12_100M_UR50S.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "raw_esm = torch.load('/dataset/ee84df8b/release/ProteinLM/pretrain/data/esm_msa1b_t12_100M_UR50S.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "megatron = raw_megatron['model']\n",
    "esm = raw_esm['model']\n",
    "megatron_lm = megatron['language_model']\n",
    "\n",
    "megatron_embed = megatron_lm['embedding']\n",
    "megatron_trans = megatron_lm['transformer']\n",
    "megatron_lm_head = megatron['lm_head']\n",
    "\n",
    "print('len(megatron_embed) + len(megatron_trans) + len(megatron_lm_head)', \\\n",
    "    len(megatron_embed) + len(megatron_trans) + len(megatron_lm_head))\n",
    "esm_embed_keys = ['encoder.sentence_encoder.msa_position_embedding', 'encoder.sentence_encoder.embed_tokens.weight', 'encoder.sentence_encoder.embed_positions.weight']\n",
    "esm_norm_before = ['encoder.sentence_encoder.emb_layer_norm_before.weight', 'encoder.sentence_encoder.emb_layer_norm_before.bias',]\n",
    "esm_trans_keys = ['encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.weight', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.bias', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.v_proj.weight', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.v_proj.bias', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.q_proj.weight', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.q_proj.bias', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.out_proj.weight', 'encoder.sentence_encoder.layers.0.column_self_attention.layer.out_proj.bias', 'encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.weight', 'encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.bias', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.k_proj.weight', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.k_proj.bias', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.v_proj.weight', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.v_proj.bias', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.q_proj.weight', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.q_proj.bias', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.out_proj.weight', 'encoder.sentence_encoder.layers.0.row_self_attention.layer.out_proj.bias', 'encoder.sentence_encoder.layers.0.row_self_attention.layer_norm.weight', 'encoder.sentence_encoder.layers.0.row_self_attention.layer_norm.bias', 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc1.weight', 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc1.bias', 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc2.weight', 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc2.bias', 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.weight', 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.bias']\n",
    "esm_lm_head_keys = ['encoder.lm_head.weight', 'encoder.lm_head.bias', 'encoder.lm_head.dense.weight', 'encoder.lm_head.dense.bias', 'encoder.lm_head.layer_norm.weight', 'encoder.lm_head.layer_norm.bias']\n",
    "esm_norm_after = ['encoder.sentence_encoder.emb_layer_norm_after.weight', 'encoder.sentence_encoder.emb_layer_norm_after.bias', ]\n",
    "\n",
    "print('len(esm_embed_keys) + 12 * len(esm_trans_keys) + len(esm_lm_head_keys)', \\\n",
    "    len(esm_embed_keys) + len(esm_norm_before) + 12 * len(esm_trans_keys) + len(esm_norm_after) + len(esm_lm_head_keys))\n",
    "\n",
    "mega_embed_keys = [i for i in megatron_embed.keys()]\n",
    "\n",
    "mega_trans_keys = [i for i in megatron_trans.keys() if 'layers.0' in i]\n",
    "mega_lm_head_keys = [i for i in megatron_lm_head.keys()] + ['final_layernorm.weight', 'final_layernorm.bias']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len(megatron_embed) + len(megatron_trans) + len(megatron_lm_head) 227\n",
      "len(esm_embed_keys) + 12 * len(esm_trans_keys) + len(esm_lm_head_keys) 325\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "megatron['language_model']['embedding'].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['word_embeddings', 'position_embeddings', 'msa_positional_embeddings', 'emb_layer_norm_before'])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "megatron['language_model']['embedding']['emb_layer_norm_before'].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "odict_keys(['weight', 'bias'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for i in range(3):\n",
    "    print(esm_embed_keys[i], esm[esm_embed_keys[i]].shape)\n",
    "\n",
    "for i in range(len(mega_embed_keys)):\n",
    "    print(mega_embed_keys[i], megatron_embed[mega_embed_keys[i]]['weight'].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "encoder.sentence_encoder.msa_position_embedding torch.Size([1, 1024, 1, 768])\n",
      "encoder.sentence_encoder.embed_tokens.weight torch.Size([33, 768])\n",
      "encoder.sentence_encoder.embed_positions.weight torch.Size([1026, 768])\n",
      "word_embeddings torch.Size([128, 768])\n",
      "position_embeddings torch.Size([1026, 768])\n",
      "msa_positional_embeddings torch.Size([1024, 768])\n",
      "emb_layer_norm_before torch.Size([768])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def assign(dst, src):\n",
    "    assert src.shape == dst.shape\n",
    "    dst[:] = src.to(dst.device)[:]\n",
    "print(mega_embed_keys, esm_embed_keys + esm_norm_before)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['word_embeddings', 'position_embeddings', 'msa_positional_embeddings', 'emb_layer_norm_before'] ['encoder.sentence_encoder.msa_position_embedding', 'encoder.sentence_encoder.embed_tokens.weight', 'encoder.sentence_encoder.embed_positions.weight', 'encoder.sentence_encoder.emb_layer_norm_before.weight', 'encoder.sentence_encoder.emb_layer_norm_before.bias']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def assign_embed():\n",
    "    m_id_e_id = {2: 0, 0: 1, 1: 32, 5: 5, 6: 25, 7: 23, 8: 13, 9: 9, 10: 18, 11: 6, 12: 21, 13: 12, 14: 15, 15: 4, 16: 20, 17: 17, 18: 28, 19: 14, 20: 16, 21: 10, 22: 8, 23: 11, 24: 26, 25: 7, 26: 22, 27: 24, 28: 19, 29: 27, 30: 30}\n",
    "    for m, e in m_id_e_id.items():\n",
    "        assign(megatron_embed['word_embeddings']['weight'][m], esm['encoder.sentence_encoder.embed_tokens.weight'][e])\n",
    "    assign(megatron_embed['word_embeddings']['weight'][: 33], esm['encoder.sentence_encoder.embed_tokens.weight'])\n",
    "    assign(megatron_embed['position_embeddings']['weight'], esm['encoder.sentence_encoder.embed_positions.weight'])\n",
    "    assign(megatron_embed['msa_positional_embeddings']['weight'], esm['encoder.sentence_encoder.msa_position_embedding'][0, :, 0])\n",
    "\n",
    "    assign(megatron_embed['emb_layer_norm_before']['weight'], esm['encoder.sentence_encoder.emb_layer_norm_before.weight'])\n",
    "    assign(megatron_embed['emb_layer_norm_before']['bias'], esm['encoder.sentence_encoder.emb_layer_norm_before.bias'])\n",
    "\n",
    "assign_embed()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "megatron_embed['word_embeddings']['weight'][0].sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(5.7588, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# torch.save(raw_megatron, '/dataset/ee84df8b/release/ProteinLM/pretrain/dump/iter_0000010/mp_rank_00/model_optim_rng.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dump = torch.load('/dataset/ee84df8b/release/ProteinLM/pretrain/dump/iter_0000010/mp_rank_00/model_optim_rng.pt')\n",
    "dump['model']['language_model']['embedding']['word_embeddings']['weight'][:2], \\\n",
    "    raw_megatron['model']['language_model']['embedding']['word_embeddings']['weight'][:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-0.2673, -0.0990,  0.1875,  ..., -0.1162,  0.0708, -0.2198],\n",
       "         [-0.0086, -0.0358,  0.0795,  ..., -0.3044,  0.0036, -0.0347]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-0.2673, -0.0990,  0.1875,  ..., -0.1162,  0.0708, -0.2198],\n",
       "         [-0.0086, -0.0358,  0.0795,  ..., -0.3044,  0.0036, -0.0347]],\n",
       "        device='cuda:0'))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "raw_megatron['model']['language_model']['embedding']['word_embeddings']['weight'][:33].sum(), \\\n",
    "    raw_megatron['model']['language_model']['embedding']['position_embeddings']['weight'].sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(nan, device='cuda:0'), tensor(47.9429, device='cuda:0'))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "esm['encoder.sentence_encoder.embed_tokens.weight'].sum(), esm['encoder.sentence_encoder.embed_positions.weight'].sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(-34.7428), tensor(47.9429))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## process layernorm before"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process Tansformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "megatron_trans['layers.0.row_input_layernorm.weight'].shape, \\\n",
    "    esm['encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.weight'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([768]))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "megatron_trans['layers.0.row_attention.query_key_value.weight'].shape, \\\n",
    "    esm['encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.weight'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([2304, 768]), torch.Size([768, 768]))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "mega_trans_keys"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['layers.0.row_input_layernorm.weight',\n",
       " 'layers.0.row_input_layernorm.bias',\n",
       " 'layers.0.col_input_layernorm.weight',\n",
       " 'layers.0.col_input_layernorm.bias',\n",
       " 'layers.0.row_attention.query_key_value.weight',\n",
       " 'layers.0.row_attention.query_key_value.bias',\n",
       " 'layers.0.row_attention.dense.weight',\n",
       " 'layers.0.row_attention.dense.bias',\n",
       " 'layers.0.col_attention.query_key_value.weight',\n",
       " 'layers.0.col_attention.query_key_value.bias',\n",
       " 'layers.0.col_attention.dense.weight',\n",
       " 'layers.0.col_attention.dense.bias',\n",
       " 'layers.0.post_attention_layernorm.weight',\n",
       " 'layers.0.post_attention_layernorm.bias',\n",
       " 'layers.0.mlp.dense_h_to_4h.weight',\n",
       " 'layers.0.mlp.dense_h_to_4h.bias',\n",
       " 'layers.0.mlp.dense_4h_to_h.weight',\n",
       " 'layers.0.mlp.dense_4h_to_h.bias']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "esm_trans_keys"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.v_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.v_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.q_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.q_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.out_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer.out_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.weight',\n",
       " 'encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.bias',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.k_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.k_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.v_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.v_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.q_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.q_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.out_proj.weight',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer.out_proj.bias',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer_norm.weight',\n",
       " 'encoder.sentence_encoder.layers.0.row_self_attention.layer_norm.bias',\n",
       " 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc1.weight',\n",
       " 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc1.bias',\n",
       " 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc2.weight',\n",
       " 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc2.bias',\n",
       " 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.weight',\n",
       " 'encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.bias']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def process_layer_i(i: int):\n",
    "    for p in ['weight', 'bias']:\n",
    "        for rc in ['row', 'col']:\n",
    "            esm_rc = 'column' if rc == 'row' else 'row'\n",
    "            assign(megatron_trans[f'layers.{i}.{rc}_input_layernorm.{p}'], esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer_norm.{p}'])\n",
    "            assign(megatron_trans[f'layers.{i}.{rc}_attention.dense.{p}'], esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.out_proj.{p}'])\n",
    "\n",
    "        assign(megatron_trans[f'layers.{i}.mlp.dense_h_to_4h.{p}'], esm[f'encoder.sentence_encoder.layers.{i}.feed_forward_layer.layer.fc1.{p}'])\n",
    "        assign(megatron_trans[f'layers.{i}.mlp.dense_4h_to_h.{p}'], esm[f'encoder.sentence_encoder.layers.{i}.feed_forward_layer.layer.fc2.{p}'])\n",
    "        assign(megatron_trans[f'layers.{i}.post_attention_layernorm.{p}'], esm[f'encoder.sentence_encoder.layers.{i}.feed_forward_layer.layer_norm.{p}'])\n",
    "\n",
    "    num_heads = 12\n",
    "    hidden_dim = 768\n",
    "    heads_dim = hidden_dim // num_heads\n",
    "    for rc in ['row', 'col']:\n",
    "        # esm_rc = rc if rc == 'row' else 'column'\n",
    "        esm_rc = 'column' if rc == 'row' else 'row'\n",
    "        # .contiguous()\n",
    "        wq = esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.q_proj.weight'].view(num_heads, heads_dim, -1)\n",
    "        wk = esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.k_proj.weight'].view(num_heads, heads_dim, -1)\n",
    "        wv = esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.v_proj.weight'].view(num_heads, heads_dim, -1)\n",
    "        bq = esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.q_proj.bias'].view(num_heads, heads_dim)\n",
    "        bk = esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.k_proj.bias'].view(num_heads, heads_dim)\n",
    "        bv = esm[f'encoder.sentence_encoder.layers.{i}.{esm_rc}_self_attention.layer.v_proj.bias'].view(num_heads, heads_dim)\n",
    "        # print(wq.shape, bq.shape)\n",
    "        # torch.Size([12, 64, 768]) torch.Size([12, 64])\n",
    "        W_mixed = torch.cat((wq, wk, wv), dim=1).reshape(hidden_dim * 3, hidden_dim)\n",
    "        B_mixed = torch.cat((bq, bk, bv), dim=1).reshape(-1)\n",
    "        assign(megatron_trans[f'layers.{i}.{rc}_attention.query_key_value.weight'], W_mixed)\n",
    "        assign(megatron_trans[f'layers.{i}.{rc}_attention.query_key_value.bias'], B_mixed)\n",
    "for i in range(12):\n",
    "    process_layer_i(i)\n",
    "torch.save(raw_megatron, '/dataset/ee84df8b/release/ProteinLM/pretrain/dump/iter_0000010/mp_rank_00/model_optim_rng.pt')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "process_layer_i(11)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "esm['encoder.sentence_encoder.embed_tokens.weight'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([33, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# megatron_trans.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "fs = \"UniRef50-xa-a2m-2017/ UniRef50-xb-a2m-2018/ UniRef50-xc-a2m-2017/ UniRef50-xd-a2m-2018/ UniRef50-xe-a2m-2017/ UniRef50-xf-a2m-2018\"\n",
    "folders = [i.strip() for i in fs.split('/')]\n",
    "folders = ['/workspace/data/' + f + '/' + f + '.json' for f in folders]\n",
    "\n",
    "print(f'/bin/cat {\" \".join(folders)} > /workspace/data/TOTAL.jsonl')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/cat /workspace/data/UniRef50-xa-a2m-2017/UniRef50-xa-a2m-2017.json /workspace/data/UniRef50-xb-a2m-2018/UniRef50-xb-a2m-2018.json /workspace/data/UniRef50-xc-a2m-2017/UniRef50-xc-a2m-2017.json /workspace/data/UniRef50-xd-a2m-2018/UniRef50-xd-a2m-2018.json /workspace/data/UniRef50-xe-a2m-2017/UniRef50-xe-a2m-2017.json /workspace/data/UniRef50-xf-a2m-2018/UniRef50-xf-a2m-2018.json > /workspace/data/TOTAL.jsonl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "isinstance(esm['encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.bias'], torch.Tensor)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "tot = 0\n",
    "def recursive_print_param_shape(model_dict):\n",
    "    for k in model_dict:\n",
    "        if isinstance(model_dict[k], torch.Tensor):\n",
    "            print(k, model_dict[k].shape)\n",
    "            global tot\n",
    "            tot += model_dict[k].numel()\n",
    "        else:\n",
    "            recursive_print_param_shape(model_dict[k])\n",
    "    # print(tot)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "recursive_print_param_shape(esm)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "encoder.sentence_encoder.msa_position_embedding torch.Size([1, 1024, 1, 768])\n",
      "encoder.sentence_encoder.embed_tokens.weight torch.Size([33, 768])\n",
      "encoder.sentence_encoder.embed_positions.weight torch.Size([1026, 768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.0.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.0.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.1.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.1.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.1.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.1.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.2.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.2.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.2.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.2.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.3.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.3.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.3.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.3.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.4.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.4.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.4.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.4.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.5.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.5.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.5.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.5.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.6.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.6.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.6.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.6.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.7.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.7.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.7.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.7.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.8.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.8.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.8.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.8.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.9.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.9.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.9.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.9.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.10.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.10.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.10.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.10.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.column_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.k_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.k_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.v_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.v_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.q_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.q_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.out_proj.weight torch.Size([768, 768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer.out_proj.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.row_self_attention.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.feed_forward_layer.layer.fc1.weight torch.Size([3072, 768])\n",
      "encoder.sentence_encoder.layers.11.feed_forward_layer.layer.fc1.bias torch.Size([3072])\n",
      "encoder.sentence_encoder.layers.11.feed_forward_layer.layer.fc2.weight torch.Size([768, 3072])\n",
      "encoder.sentence_encoder.layers.11.feed_forward_layer.layer.fc2.bias torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.feed_forward_layer.layer_norm.weight torch.Size([768])\n",
      "encoder.sentence_encoder.layers.11.feed_forward_layer.layer_norm.bias torch.Size([768])\n",
      "encoder.sentence_encoder.emb_layer_norm_before.weight torch.Size([768])\n",
      "encoder.sentence_encoder.emb_layer_norm_before.bias torch.Size([768])\n",
      "encoder.sentence_encoder.emb_layer_norm_after.weight torch.Size([768])\n",
      "encoder.sentence_encoder.emb_layer_norm_after.bias torch.Size([768])\n",
      "encoder.lm_head.weight torch.Size([33, 768])\n",
      "encoder.lm_head.bias torch.Size([33])\n",
      "encoder.lm_head.dense.weight torch.Size([768, 768])\n",
      "encoder.lm_head.dense.bias torch.Size([768])\n",
      "encoder.lm_head.layer_norm.weight torch.Size([768])\n",
      "encoder.lm_head.layer_norm.bias torch.Size([768])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "tot"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "115641633"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "recursive_print_param_shape(megatron)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight torch.Size([128, 768])\n",
      "weight torch.Size([1024, 768])\n",
      "layers.0.row_input_layernorm.weight torch.Size([768])\n",
      "layers.0.row_input_layernorm.bias torch.Size([768])\n",
      "layers.0.col_input_layernorm.weight torch.Size([768])\n",
      "layers.0.col_input_layernorm.bias torch.Size([768])\n",
      "layers.0.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.0.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.0.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.0.row_attention.dense.bias torch.Size([768])\n",
      "layers.0.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.0.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.0.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.0.col_attention.dense.bias torch.Size([768])\n",
      "layers.0.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.0.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.0.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.0.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.0.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.0.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.1.row_input_layernorm.weight torch.Size([768])\n",
      "layers.1.row_input_layernorm.bias torch.Size([768])\n",
      "layers.1.col_input_layernorm.weight torch.Size([768])\n",
      "layers.1.col_input_layernorm.bias torch.Size([768])\n",
      "layers.1.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.1.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.1.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.1.row_attention.dense.bias torch.Size([768])\n",
      "layers.1.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.1.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.1.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.1.col_attention.dense.bias torch.Size([768])\n",
      "layers.1.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.1.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.1.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.1.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.1.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.1.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.2.row_input_layernorm.weight torch.Size([768])\n",
      "layers.2.row_input_layernorm.bias torch.Size([768])\n",
      "layers.2.col_input_layernorm.weight torch.Size([768])\n",
      "layers.2.col_input_layernorm.bias torch.Size([768])\n",
      "layers.2.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.2.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.2.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.2.row_attention.dense.bias torch.Size([768])\n",
      "layers.2.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.2.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.2.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.2.col_attention.dense.bias torch.Size([768])\n",
      "layers.2.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.2.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.2.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.2.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.2.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.2.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.3.row_input_layernorm.weight torch.Size([768])\n",
      "layers.3.row_input_layernorm.bias torch.Size([768])\n",
      "layers.3.col_input_layernorm.weight torch.Size([768])\n",
      "layers.3.col_input_layernorm.bias torch.Size([768])\n",
      "layers.3.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.3.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.3.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.3.row_attention.dense.bias torch.Size([768])\n",
      "layers.3.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.3.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.3.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.3.col_attention.dense.bias torch.Size([768])\n",
      "layers.3.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.3.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.3.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.3.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.3.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.3.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.4.row_input_layernorm.weight torch.Size([768])\n",
      "layers.4.row_input_layernorm.bias torch.Size([768])\n",
      "layers.4.col_input_layernorm.weight torch.Size([768])\n",
      "layers.4.col_input_layernorm.bias torch.Size([768])\n",
      "layers.4.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.4.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.4.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.4.row_attention.dense.bias torch.Size([768])\n",
      "layers.4.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.4.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.4.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.4.col_attention.dense.bias torch.Size([768])\n",
      "layers.4.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.4.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.4.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.4.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.4.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.4.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.5.row_input_layernorm.weight torch.Size([768])\n",
      "layers.5.row_input_layernorm.bias torch.Size([768])\n",
      "layers.5.col_input_layernorm.weight torch.Size([768])\n",
      "layers.5.col_input_layernorm.bias torch.Size([768])\n",
      "layers.5.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.5.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.5.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.5.row_attention.dense.bias torch.Size([768])\n",
      "layers.5.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.5.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.5.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.5.col_attention.dense.bias torch.Size([768])\n",
      "layers.5.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.5.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.5.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.5.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.5.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.5.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.6.row_input_layernorm.weight torch.Size([768])\n",
      "layers.6.row_input_layernorm.bias torch.Size([768])\n",
      "layers.6.col_input_layernorm.weight torch.Size([768])\n",
      "layers.6.col_input_layernorm.bias torch.Size([768])\n",
      "layers.6.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.6.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.6.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.6.row_attention.dense.bias torch.Size([768])\n",
      "layers.6.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.6.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.6.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.6.col_attention.dense.bias torch.Size([768])\n",
      "layers.6.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.6.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.6.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.6.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.6.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.6.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.7.row_input_layernorm.weight torch.Size([768])\n",
      "layers.7.row_input_layernorm.bias torch.Size([768])\n",
      "layers.7.col_input_layernorm.weight torch.Size([768])\n",
      "layers.7.col_input_layernorm.bias torch.Size([768])\n",
      "layers.7.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.7.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.7.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.7.row_attention.dense.bias torch.Size([768])\n",
      "layers.7.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.7.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.7.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.7.col_attention.dense.bias torch.Size([768])\n",
      "layers.7.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.7.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.7.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.7.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.7.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.7.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.8.row_input_layernorm.weight torch.Size([768])\n",
      "layers.8.row_input_layernorm.bias torch.Size([768])\n",
      "layers.8.col_input_layernorm.weight torch.Size([768])\n",
      "layers.8.col_input_layernorm.bias torch.Size([768])\n",
      "layers.8.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.8.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.8.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.8.row_attention.dense.bias torch.Size([768])\n",
      "layers.8.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.8.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.8.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.8.col_attention.dense.bias torch.Size([768])\n",
      "layers.8.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.8.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.8.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.8.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.8.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.8.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.9.row_input_layernorm.weight torch.Size([768])\n",
      "layers.9.row_input_layernorm.bias torch.Size([768])\n",
      "layers.9.col_input_layernorm.weight torch.Size([768])\n",
      "layers.9.col_input_layernorm.bias torch.Size([768])\n",
      "layers.9.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.9.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.9.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.9.row_attention.dense.bias torch.Size([768])\n",
      "layers.9.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.9.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.9.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.9.col_attention.dense.bias torch.Size([768])\n",
      "layers.9.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.9.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.9.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.9.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.9.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.9.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.10.row_input_layernorm.weight torch.Size([768])\n",
      "layers.10.row_input_layernorm.bias torch.Size([768])\n",
      "layers.10.col_input_layernorm.weight torch.Size([768])\n",
      "layers.10.col_input_layernorm.bias torch.Size([768])\n",
      "layers.10.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.10.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.10.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.10.row_attention.dense.bias torch.Size([768])\n",
      "layers.10.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.10.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.10.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.10.col_attention.dense.bias torch.Size([768])\n",
      "layers.10.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.10.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.10.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.10.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.10.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.10.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "layers.11.row_input_layernorm.weight torch.Size([768])\n",
      "layers.11.row_input_layernorm.bias torch.Size([768])\n",
      "layers.11.col_input_layernorm.weight torch.Size([768])\n",
      "layers.11.col_input_layernorm.bias torch.Size([768])\n",
      "layers.11.row_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.11.row_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.11.row_attention.dense.weight torch.Size([768, 768])\n",
      "layers.11.row_attention.dense.bias torch.Size([768])\n",
      "layers.11.col_attention.query_key_value.weight torch.Size([2304, 768])\n",
      "layers.11.col_attention.query_key_value.bias torch.Size([2304])\n",
      "layers.11.col_attention.dense.weight torch.Size([768, 768])\n",
      "layers.11.col_attention.dense.bias torch.Size([768])\n",
      "layers.11.post_attention_layernorm.weight torch.Size([768])\n",
      "layers.11.post_attention_layernorm.bias torch.Size([768])\n",
      "layers.11.mlp.dense_h_to_4h.weight torch.Size([3072, 768])\n",
      "layers.11.mlp.dense_h_to_4h.bias torch.Size([3072])\n",
      "layers.11.mlp.dense_4h_to_h.weight torch.Size([768, 3072])\n",
      "layers.11.mlp.dense_4h_to_h.bias torch.Size([768])\n",
      "final_layernorm.weight torch.Size([768])\n",
      "final_layernorm.bias torch.Size([768])\n",
      "bias torch.Size([128])\n",
      "dense.weight torch.Size([768, 768])\n",
      "dense.bias torch.Size([768])\n",
      "layernorm.weight torch.Size([768])\n",
      "layernorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "tot"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tot' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_208763/144942766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tot' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "!factor 741793 # 115641633 - 114899840"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "741793: 13 43 1327\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}